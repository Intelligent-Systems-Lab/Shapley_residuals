{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import bisect\n",
    "import math\n",
    "import json\n",
    "class Hypercube:                #超立方體\n",
    "    '''\n",
    "    A class to create a hypercube object which stores values on vertices\n",
    "    and values on the edges between neighboring vertices\n",
    "    '''    \n",
    "    #輸入維度、(點鍵值)、(點值)\n",
    "    def __init__(self, n_vertices, vertex_keys = None, vertex_values = None):   \n",
    "        self.n_vertices = n_vertices\n",
    "        self.v_num = 2**self.n_vertices\n",
    "        self.V = [np.array([])] + all_subsets(n_vertices)   #所有子集包含空集，即所有點\n",
    "        self.V_value = {str(v) : 0. for v in self.V}         #所有點值  先設為0\n",
    "        self.E = []\n",
    "        self.E_value = {}\n",
    "        self.partial_gradient = {vertex : {} for vertex in range(n_vertices)}   #各個維度的partial gradient\n",
    "        self.matrix = np.full((self.v_num,self.v_num),np.nan)                   #原始立方體對應到的矩陣\n",
    "        self.partial_gradient_matrix = np.full((self.v_num,self.v_num),np.nan)  #對特定 feature的partial gradient\n",
    "        self.vi_matrix = np.full((self.v_num,self.v_num),np.nan)                #v_i對應到的矩陣\n",
    "        self.res_matrix = np.full((self.v_num,self.v_num),np.nan)               #residual的matrix\n",
    "        self.sv = 0\n",
    " \n",
    "    def set_vertex_values(self, vertex_values):         #設置點值\n",
    "        for v in vertex_values:                         #用鍵值來做查找\n",
    "            self.V_value[v] = vertex_values[v]\n",
    "            \n",
    "        # edge values are the differences between neighboring vertex values\n",
    "        #self._calculate_edges()\n",
    "        \n",
    "    def _calculate_edges(self):                 #計算邊值\n",
    "        \n",
    "        # calculate the usual gradients: the difference between neighboring edges\n",
    "        for i, v in enumerate(self.V):\n",
    "            for _v in self.V[i+1:]:\n",
    "                if self._vertices_form_a_valid_edge(v, _v):\n",
    "                    self.E.append((v, _v))\n",
    "                    self.E_value[str((v, _v))] = self.V_value[str(_v)] - self.V_value[str(v)]\n",
    "        \n",
    "        # calculate partial gradients\n",
    "        for vertex in range(self.n_vertices):\n",
    "            self.partial_gradient[vertex] = self.E_value.copy()\n",
    "            for v, _v in self.E:\n",
    "                is_relevant_edge_for_partial_gradient = (vertex in v and vertex not in _v) or (vertex in _v and vertex not in v)\n",
    "                if not is_relevant_edge_for_partial_gradient:\n",
    "                    self.partial_gradient[vertex][str((v, _v))] = 0.\n",
    "            \n",
    "    def _vertices_form_a_valid_edge(self, v, _v):       #檢查交集和是否相鄰\n",
    "        # vertices are neighbors in a hypercube\n",
    "        # if they differ by exactly one element\n",
    "        differ_in_size_by_1 = (abs(len(v) - len(_v)) == 1)      #差距是1\n",
    "    \n",
    "        the_intersection = np.intersect1d(v, _v)                #兩個集合的交集\n",
    "        #print(type(v[0]),type(_v[0]),type(the_intersection[0]))\n",
    "        intersection_is_nonempty = len(the_intersection) > 0 or len(v)==0 or len(_v) == 0   #交集大於0或v,_v是空集\n",
    "        is_intersection = False                         \n",
    "        if len(the_intersection)>0:                              \n",
    "            if len(the_intersection)==len(v) or len(the_intersection)==len(_v):\n",
    "                is_intersection = True\n",
    "        else:\n",
    "            if len(v)==0 and len(_v)==1:\n",
    "                is_intersection = True\n",
    "       # print(is_intersection)\n",
    "        return differ_in_size_by_1 and intersection_is_nonempty and is_intersection\n",
    "    \n",
    "    #create matrix for Hypercube\n",
    "    def trans_to_matrix(self,feature_i):                #超立方體轉換成矩陣\n",
    "        for i, v in enumerate(self.V):                  #對立方體上所有點\n",
    "            for j,_v in enumerate(self.V[i+1:]):        \n",
    "                if self._vertices_form_a_valid_edge(v, _v):     #是否相交\n",
    "                    self.matrix[i][i+j+1] = self.V_value[str(_v)] - self.V_value[str(v)]    #獲得matrix\n",
    "                    self.matrix[i+j+1][i] = self.V_value[str(v)] - self.V_value[str(_v)]\n",
    "        \n",
    "            \n",
    "        self.partial_gradient_matrix = self.matrix.copy()       #把原始立方體複製下來，保留i方向\n",
    "        self.my_list = []\n",
    "        self.partial_gradient_vector = np.array([])\n",
    "        self.weight_vector = np.array([])\n",
    "        self.partial_gradient_norm = 0\n",
    "        for j, v in enumerate(self.V):\n",
    "            for k,_v in enumerate(self.V[j+1:]):\n",
    "                if self._vertices_form_a_valid_edge(v, _v):     #如果是邊\n",
    "                    is_relevant_edge_for_partial_gradient = (feature_i in v and feature_i not in _v) or (feature_i in _v and feature_i not in v)\n",
    "                    if not is_relevant_edge_for_partial_gradient:           #如果 特徵i不在v 或_v，把那條邊設成0\n",
    "                        self.partial_gradient_matrix[j][j+k+1] = 0.\n",
    "                        self.partial_gradient_matrix[j+k+1][j] = 0.\n",
    "                    else:\n",
    "                        self.partial_gradient_norm += (self.partial_gradient_matrix[j][j+k+1])**2       # norm\n",
    "                        self.partial_gradient_vector = np.append(self.partial_gradient_vector,self.partial_gradient_matrix[j][j+k+1])      #比較向量1\n",
    "                        subset_len = len(v)\n",
    "                        weight = 1./(math.comb(self.n_vertices,subset_len)*(self.n_vertices-subset_len))            #Shapley value權重參數\n",
    "                        self.weight_vector = np.append(self.weight_vector,weight)\n",
    "                        self.my_list.append((j,j+k+1))          \n",
    "        #self.vi[feature_i] = self.shapley_residuals_in_matrix()\n",
    "        #self.vi[i] = self.shapley_residuals_in_matrix\n",
    "    \n",
    "    #minimize the function (gradient_x - partial_gradient_i)^2 最小化l2_norm\n",
    "    def shapley_residuals_in_matrix(self):\n",
    "            derivative_i  = np.full((self.v_num,self.v_num),0.)      #計算微分後得到的方程式矩陣 A\n",
    "            b_i = np.array([0.]*self.v_num)                          #得到Ax = b 的 b向量值\n",
    "            for j  in range(self.v_num):                            #對矩陣的每個點\n",
    "                for k in range(self.v_num):\n",
    "                    if np.isnan(self.partial_gradient_matrix[j][k]):    #不用計算nan\n",
    "                        continue\n",
    "                    #elif j == 0 or k ==0:                               #如果是跟原點相鄰\n",
    "                     #   derivative_i[j][j] += 1.                       #係數+1\n",
    "                      #  b_i[j] += - self.partial_gradient_matrix[j][k]     #x_j-x_i-partial_gradient\n",
    "                    else:                                               \n",
    "                        derivative_i[j][j] += 1.                         \n",
    "                        derivative_i[j][k] += -1.\n",
    "                        b_i[j] += - self.partial_gradient_matrix[j][k]\n",
    "            A = derivative_i[1:,1:]                                    #只要x_i for i!=0\n",
    "            B = b_i[1:]                                                #保留b_i\n",
    "            res = 0.\n",
    "            A_inverse = np.linalg.inv(A)                               #算inverse matrix\n",
    "            vi = np.insert(np.dot(A_inverse,B),0,0.)                    #vi  = b/A #開頭補0\n",
    "            vi_V =  [np.array([])] + all_subsets(self.n_vertices)\n",
    "            vi_V_value = {str(v) : 0. for v in vi_V} \n",
    "            for k,v in enumerate(vi_V):               \n",
    "                vi_V_value[str(v)] = vi[k]\n",
    "            self.vi_vector = np.array([])                              #比較向量#2\n",
    "            for i, v in enumerate(vi_V):\n",
    "                for j,_v in enumerate(vi_V[i+1:]):\n",
    "                    if self._vertices_form_a_valid_edge(v, _v):\n",
    "                        self.vi_matrix[i][i+j+1] = vi_V_value[str(_v)] - vi_V_value[str(v)]\n",
    "                        self.vi_matrix[i+j+1][i] = vi_V_value[str(v)] - vi_V_value[str(_v)]\n",
    "                        self.res_matrix[i][i+j+1] = self.partial_gradient_matrix[i][i+j+1] - self.vi_matrix[i][i+j+1]\n",
    "                        self.res_matrix[i+j+1][i] = self.partial_gradient_matrix[i+j+1][i] - self.vi_matrix[i+j+1][i]\n",
    "                        res += (self.res_matrix[i][i+j+1])**2\n",
    "                        if (i,i+j+1) in self.my_list:\n",
    "                            self.vi_vector = np.append(self.vi_vector,self.vi_matrix[i][i+j+1])     #比較向量#2\n",
    "            #print(self.my_list)\n",
    "            #print(self.partial_gradient_vector,self.vi_vector)\n",
    "            vector_A = self.weight_vector*self.partial_gradient_vector\n",
    "            vector_B = self.weight_vector*self.vi_vector\n",
    "            cos_sim = self.cos_sim(vector_A,vector_B)\n",
    "            print('cos_sim = ',cos_sim)\n",
    "            self.sv += vi[-1]\n",
    "            print('shapley_value: ',vi[-1],'residual sum: ',res)\n",
    "            #print(self.sv)\n",
    "            #print('residuals_sum:',res,'shapley_value: ',vi)\n",
    "            res =  (res/self.partial_gradient_norm)**0.5\n",
    "            print('norm: ',res)\n",
    "            \n",
    "            return vi_V_value\n",
    "    \n",
    "    def cos_sim(self,a,b):\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        similarity = dot_product / (norm_a * norm_b)\n",
    "        return similarity\n",
    "            \n",
    "####################     \n",
    "def save_json(dic,feature_name):\n",
    "    filename = f\"dic_{feature_name}.json\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(dic, json_file)\n",
    "            \n",
    "        \n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "\n",
    "def get_residual(old_cube, new_cube, vertex):   #計算殘差\n",
    "    '''\n",
    "    returns: residual dictionary\n",
    "        \n",
    "        { edge : ▼_player_v[edge] - ▼v_player[edge] for edge in old_cube }\n",
    "    '''\n",
    "    assert set(old_cube.E_value.keys()) == set(new_cube.E_value.keys())     #判斷兩個字典中鍵值的組合是否相同。assert:\n",
    "    res = {}\n",
    "    for e in old_cube.E_value.keys():\n",
    "        res[e] = old_cube.partial_gradient[vertex][e] - new_cube.E_value[e] #對應某特徵的邊相減 即gradient_i_v - gradient_v_i(殘差)\n",
    "    return res\n",
    "count = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "#記得添加number of vertex\n",
    "def residual_norm(old_cube, vertex_values, vertex,num_features):     #old_cube是原本的SHAP得到的立方體\n",
    "    '''\n",
    "    old_cube: v, our game\n",
    "    vertex: player\n",
    "    vertex_values: v_player, proposed game\n",
    "    \n",
    "    assumes that the order of the values in vertex_values align with the order of the values in old_cube.V\n",
    "    \n",
    "    returns: || ▼_player_v - ▼v_player ||\n",
    "    '''\n",
    "    if count[vertex]==0 :\n",
    "        count[vertex] += 1\n",
    "    new_cube = Hypercube(num_features)\n",
    "    new_cube.set_vertex_values({str(_vertex) : vertex_values[j] for j, _vertex in enumerate(old_cube.V)})   #將數值設定成0.5\n",
    "    return np.sum([(r)**2 for r in get_residual(old_cube, new_cube, vertex).values()]), get_residual(old_cube, new_cube, vertex).values() #計算所有residual造成的影響加總\n",
    "#改一下參數\n",
    "def compute_residuals_v(old_cube,vertex_of_v_i_cube,_v,num_features):            #(instance cube,算出來的v_i cube,這個cube的指定feature)\n",
    "    new_vertex =  np.append(np.array(0), vertex_of_v_i_cube)\n",
    "    new_c = Hypercube(num_features)\n",
    "    coalitions = [np.array([])] + all_subsets(num_features)\n",
    "    b = {}\n",
    "    for i, coalition in enumerate(coalitions):\n",
    "        b[str(coalition)] = new_vertex[i]\n",
    "    new_c.set_vertex_values(b)\n",
    "    res = get_residual(old_cube,new_c,_v)\n",
    "    return(res.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(5):\n",
    "    for k  in range(3):\n",
    "        a.append((i,k))\n",
    "if (4,2) in a:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from itertools import combinations\n",
    "\n",
    "def generate_all_subsets(num):\n",
    "    num_set = [i for i in range(num)]\n",
    "    return [np.array(s) for r in range(num+1) for s in combinations(num_set, r) ]\n",
    "def all_subsets(n_elts):\n",
    "        #returns a list of 2^{n_elts} lists\n",
    "        #each a different subset of {1, 2,...,n_elts}\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "a  = generate_all_subsets(5)\n",
    "\n",
    "b = [np.array([])]+all_subsets(5)\n",
    "print(a)\n",
    "print(b)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process import read_selected_data, get_y,  split_data, compute_class_weights\n",
    "from dataset import BertDataset\n",
    "from model import BertClassifier\n",
    "from training import train_model\n",
    "from utils import draw_pics, initial_record\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "total_params = 14\n",
    "csv_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/processed_data/v014_stage_1.csv'\n",
    "json_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/controllable_para_v014_14.json'\n",
    "tool_name = 'ASCVD'\n",
    "epochs = 50000\n",
    "lr = 1e-5\n",
    "batch_size = 1024\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "save_folder_name = 'stage-1-param_'+str(total_params)+'-batch_'+str(batch_size)+'-lr_'+str(lr)\n",
    "with open(json_file_path, 'r') as f:\n",
    "    params = json.load(f)\n",
    "    f.close()\n",
    "s1_df = pd.read_csv(csv_file_path)\n",
    "#s1_df.shape\n",
    "all_key = list(params[tool_name]) \n",
    "params_list = []                #取得json檔內的特徵\n",
    "for key in all_key:\n",
    "    all_param = params[tool_name][key]\n",
    "    if(type(all_param) == list):\n",
    "        for param in all_param:\n",
    "             params_list.append(param)\n",
    "    else:\n",
    "        params_list.append(all_param)\n",
    "    \n",
    "# 取得Json檔內包含的特徵\n",
    "s1_df = s1_df[params_list] \n",
    "#print(params)\n",
    "s1_df.head(10)\n",
    "feature_df = s1_df.drop(['DFT_CNT'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(feature_df)\n",
    "nf_df = pd.DataFrame(X_standardized)\n",
    "nf_df\n",
    "param_group = [] # [2,2,4,2]\n",
    "all_key = list(params[tool_name]) # ['EQ', 'PUMP', 'CH', 'VENT', 'y']\n",
    "all_key.remove('y')\n",
    "\n",
    "for key in all_key:\n",
    "    all_value = params[tool_name][key]\n",
    "    param_group.append(len(all_value))\n",
    "param_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_zero(df, tool_name, total_params, flag, params=params): \n",
    "    # 將一維參數matrix擴展為4維\n",
    "    data_arr = df.to_numpy()\n",
    "    result = []\n",
    "    for i in range(len(data_arr)):\n",
    "        arr_index = 0\n",
    "        empty_arr = np.zeros((4,total_params)) # chamber數 * 總參數數量\n",
    "        param_group_cp = param_group.copy()\n",
    "        for j in range(len(empty_arr)):\n",
    "            while(param_group_cp[j] > 0):\n",
    "                empty_arr[j][arr_index] = data_arr[i][arr_index]\n",
    "                param_group_cp[j] -= 1\n",
    "                arr_index += 1\n",
    "        \n",
    "        if(flag == 1): # bert.py使用\n",
    "            result.append(empty_arr)\n",
    "        if(flag == 2): # bert_du.py使用\n",
    "            result.append(empty_arr.tolist())\n",
    "    \n",
    "    if(flag == 1): # bert.py使用\n",
    "        result = pd.DataFrame({'X': [result[i] for i in range(len(result))]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_df_4d = padding_zero(nf_df,tool_name,total_params,flag=1)\n",
    "nf_df_4d_object = nf_df_4d.to_numpy()\n",
    "nf_df_4d_list = []\n",
    "for i in range(len(nf_df_4d_object)):\n",
    "    nf_df_4d_list.append(nf_df_4d_object[i][0])\n",
    "nf_df_4d_arr = np.array(nf_df_4d_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料模型預測及平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "s1_model_path = '/hcds_vol/private/luffy/GANGAN-master/model/predictor/stage_1_checkpoint.pth'\n",
    "s1_model =  torch.load(s1_model_path).to(device)\n",
    "s1_model.eval()\n",
    "nf_df_4d_tensor = torch.tensor(nf_df_4d_arr,dtype=torch.float)\n",
    "dataset = TensorDataset(nf_df_4d_tensor)\n",
    "batch_size = 256\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "        batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "        batch_output = s1_model(batch_data)\n",
    "        probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "        outputs += probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得模型平均和對應output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_arr = np.array([output.cpu().numpy()[0] for output in outputs])\n",
    "output_df = pd.DataFrame({'Output': output_arr})\n",
    "new_df = pd.concat([feature_df,output_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(data):\n",
    "    if 'Output' in data.columns:\n",
    "        data = data.drop(columns=['Output'])\n",
    "    data_standardized = scaler.transform(data)\n",
    "    data_standardized_df = pd.DataFrame(data_standardized)\n",
    "    data_4d = padding_zero(data_standardized_df,tool_name,total_params,flag=1)\n",
    "    data_4d_object =  data_4d.to_numpy()\n",
    "    data_4d_list = []\n",
    "    for i in range(len(data_4d_object)):\n",
    "        data_4d_list.append(data_4d_object[i][0])\n",
    "    data_4d_arr = np.array(data_4d_list)\n",
    "    data_4d_tensor = torch.tensor(data_4d_arr,dtype=torch.float)\n",
    "    my_dataset = TensorDataset(data_4d_tensor)\n",
    "    batch_size = 256\n",
    "    my_loader = DataLoader(my_dataset, batch_size=batch_size,num_workers=4)\n",
    "    data_output = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in my_loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "            batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "            batch_output = s1_model(batch_data)\n",
    "            probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "            data_output += probs\n",
    "    data_output_arr = np.array([output.cpu().numpy()[0] for output in data_output])\n",
    "    data_expectation_out = data_output_arr.mean()\n",
    "    return data_expectation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import warnings\n",
    "import itertools\n",
    "# 過濾掉FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "para_num = 14\n",
    "AUO_coalitions = [np.array([])] + all_subsets(para_num) \n",
    "coalition_estimated_values = {}\n",
    "instance = new_df.iloc[3]\n",
    "#mean_exp = new_df['Output'].mean()\n",
    "flag=0\n",
    "count_n = 1\n",
    "selected_data = new_df.copy()\n",
    "selected_data = selected_data[(selected_data['PUMP_low']<20000) & \n",
    "                              (selected_data['PUMP_high']>20000) & \n",
    "                              (selected_data['VENT_low']<10000) & \n",
    "                              (selected_data['VENT_high']>10000) &\n",
    "                              (selected_data['NH3_TREAT_-RF_FREQ-max']>13800)&\n",
    "                              (selected_data['NH3_TREAT_-RF_FREQ-mean']<13800)\n",
    "                                                            ]\n",
    "print(len(selected_data))\n",
    "sample_df = selected_data.sample(n=1000,random_state=42)\n",
    "instance.to_csv('instance_3d.csv',index=True)\n",
    "sample_df.to_csv('background_dataset_2.csv',index=True)\n",
    "mean_exp = sample_df['Output'].mean()\n",
    "print(instance['Output'])\n",
    "for coalition in AUO_coalitions:\n",
    "    synth = sample_df.copy()                   #用copy()才不會去更改到原始的dataframe\n",
    "    if len(coalition)!=0:\n",
    "        #print(synth.iloc[:,coalition],instance[coalition])\n",
    "        synth.iloc[:,coalition] = instance.iloc[coalition]\n",
    "        \n",
    "        '''if len(coalition)==3 and flag==0:\n",
    "            print(instance)\n",
    "            print(synth.head(5))\n",
    "            flag=1'''\n",
    "        #if (2 in coalition and 3 not in coalition):\n",
    "        #    PUMP_high = instance.iloc[2]\n",
    "        #    synth = synth[synth.iloc[:,3]<PUMP_high]\n",
    "            #print('good')\n",
    "            \n",
    "    #if count_n==100:\n",
    "        #print('資料集長度:',len(synth))\n",
    "        #print('feature數:',len(coalition))\n",
    "        #print('資料: ',synth)\n",
    "        #print(coalition)\n",
    "        #count_n = 0\n",
    "\n",
    "    count_n += 1\n",
    "    Exp = send_to_model(synth)\n",
    "    impact = Exp - mean_exp\n",
    "\n",
    "    coalition_estimated_values[str(coalition)] = impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUOcube = Hypercube(para_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUOcube.set_vertex_values(coalition_estimated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i  in range(para_num):\n",
    "    AUOcube.trans_to_matrix(feature_i=i)\n",
    "    res = AUOcube.shapley_residuals_in_matrix()\n",
    "    #print(new_df.columns[i],'residuals_of_feature',i)\n",
    "    save_json(res,feature_name=params_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
