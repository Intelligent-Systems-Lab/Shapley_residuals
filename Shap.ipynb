{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "class Hypercube:                #超立方體\n",
    "    '''\n",
    "    A class to create a hypercube object which stores values on vertices\n",
    "    and values on the edges between neighboring vertices\n",
    "    '''    \n",
    "    #輸入維度、(點鍵值)、(點值)\n",
    "    def __init__(self, n_vertices, vertex_keys = None, vertex_values = None):   \n",
    "        self.n_vertices = n_vertices\n",
    "        self.v_num = 2**self.n_vertices\n",
    "        self.V = [np.array([])] + all_subsets(n_vertices)   #所有子集包含空集，即所有點\n",
    "        self.V_value = {str(v) : 0 for v in self.V}         #所有點值  先設為0\n",
    "        self.E = []                                         #所有邊\n",
    "        self.E_value = {}                                   #邊值\n",
    "        self.partial_gradient = {vertex : {} for vertex in range(n_vertices)}   #各個維度的partial gradient\n",
    "        self.matrix = np.full((self.v_num,self.v_num),np.nan)\n",
    "        self.partial_gradient_matrix = [np.full((self.v_num,self.v_num),np.nan) for _ in range(self.n_vertices)]\n",
    "    def set_vertex_values(self, vertex_values):         #設置點值\n",
    "        for v in vertex_values:                         #用鍵值來做查找\n",
    "            self.V_value[v] = vertex_values[v]\n",
    "            \n",
    "        # edge values are the differences between neighboring vertex values\n",
    "        self._calculate_edges()\n",
    "        \n",
    "    def _calculate_edges(self):                 #計算邊值\n",
    "        \n",
    "        # calculate the usual gradients: the difference between neighboring edges\n",
    "        for i, v in enumerate(self.V):\n",
    "            for _v in self.V[i+1:]:\n",
    "                if self._vertices_form_a_valid_edge(v, _v):\n",
    "                    self.E.append((v, _v))\n",
    "                    self.E_value[str((v, _v))] = self.V_value[str(_v)] - self.V_value[str(v)]\n",
    "        \n",
    "        # calculate partial gradients\n",
    "        for vertex in range(self.n_vertices):\n",
    "            self.partial_gradient[vertex] = self.E_value.copy()\n",
    "            for v, _v in self.E:\n",
    "                is_relevant_edge_for_partial_gradient = (vertex in v and vertex not in _v) or (vertex in _v and vertex not in v)\n",
    "                if not is_relevant_edge_for_partial_gradient:\n",
    "                    self.partial_gradient[vertex][str((v, _v))] = 0\n",
    "            \n",
    "    def _vertices_form_a_valid_edge(self, v, _v):       #檢查交集和是否相鄰\n",
    "        # vertices are neighbors in a hypercube\n",
    "        # if they differ by exactly one element\n",
    "        differ_in_size_by_1 = (abs(len(v) - len(_v)) == 1)\n",
    "    \n",
    "        the_intersection = np.intersect1d(v, _v)                #兩個集合的交集\n",
    "        #print(type(v[0]),type(_v[0]),type(the_intersection[0]))\n",
    "        intersection_is_nonempty = len(the_intersection) > 0 or len(v)==0 or len(_v) == 0\n",
    "        is_intersection = False\n",
    "        if len(the_intersection)>0:\n",
    "            if len(the_intersection)==len(v) or len(the_intersection)==len(_v):\n",
    "                is_intersection = True\n",
    "        else:\n",
    "            if len(v)==0 and len(_v)==1:\n",
    "                is_intersection = True\n",
    "       # print(is_intersection)\n",
    "        return differ_in_size_by_1 and intersection_is_nonempty and is_intersection\n",
    "\n",
    "    def trans_to_matrix(self):\n",
    "       for i, v in enumerate(self.V):\n",
    "            for j,_v in enumerate(self.V[i+1:]):\n",
    "                if self._vertices_form_a_valid_edge(v, _v):\n",
    "                    self.matrix[i][i+j+1] = self.V_value[str(_v)] - self.V_value[str(v)]\n",
    "                    self.matrix[i+j+1][i] = self.V_value[str(v)] - self.V_value[str(_v)]\n",
    "        for i in range(self.n_vertices):\n",
    "            self.partial_gradient_matrix[i] = self.matrix.copy()\n",
    "            for v, _v in self.E:\n",
    "                is_relevant_edge_for_partial_gradient = (vertex in v and vertex not in _v) or (vertex in _v and vertex not in v)\n",
    "                if not is_relevant_edge_for_partial_gradient:\n",
    "                    self.partial_gradient[vertex][str((v, _v))] = 0\n",
    "                        \n",
    "        \n",
    "    #def trans_partial_matrix(self):\n",
    "    def\n",
    "        \n",
    "####################\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "\n",
    "def get_residual(old_cube, new_cube, vertex):   #計算殘差\n",
    "    '''\n",
    "    returns: residual dictionary\n",
    "        \n",
    "        { edge : ▼_player_v[edge] - ▼v_player[edge] for edge in old_cube }\n",
    "    '''\n",
    "    assert set(old_cube.E_value.keys()) == set(new_cube.E_value.keys())     #判斷兩個字典中鍵值的組合是否相同。assert:\n",
    "    res = {}\n",
    "    for e in old_cube.E_value.keys():\n",
    "        res[e] = old_cube.partial_gradient[vertex][e] - new_cube.E_value[e] #對應某特徵的邊相減 即gradient_i_v - gradient_v_i(殘差)\n",
    "    return res\n",
    "count = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "#記得添加number of vertex\n",
    "def residual_norm(old_cube, vertex_values, vertex,num_features):     #old_cube是原本的SHAP得到的立方體\n",
    "    '''\n",
    "    old_cube: v, our game\n",
    "    vertex: player\n",
    "    vertex_values: v_player, proposed game\n",
    "    \n",
    "    assumes that the order of the values in vertex_values align with the order of the values in old_cube.V\n",
    "    \n",
    "    returns: || ▼_player_v - ▼v_player ||\n",
    "    '''\n",
    "    if count[vertex]==0 :\n",
    "        count[vertex] += 1\n",
    "    new_cube = Hypercube(num_features)\n",
    "    new_cube.set_vertex_values({str(_vertex) : vertex_values[j] for j, _vertex in enumerate(old_cube.V)})   #將數值設定成0.5\n",
    "    return np.sum([(r)**2 for r in get_residual(old_cube, new_cube, vertex).values()]), get_residual(old_cube, new_cube, vertex).values() #計算所有residual造成的影響加總\n",
    "#改一下參數\n",
    "def compute_residuals_v(old_cube,vertex_of_v_i_cube,_v,num_features):            #(instance cube,算出來的v_i cube,這個cube的指定feature)\n",
    "    new_vertex =  np.append(np.array(0), vertex_of_v_i_cube)\n",
    "    new_c = Hypercube(num_features)\n",
    "    coalitions = [np.array([])] + all_subsets(num_features)\n",
    "    b = {}\n",
    "    for i, coalition in enumerate(coalitions):\n",
    "        b[str(coalition)] = new_vertex[i]\n",
    "    new_c.set_vertex_values(b)\n",
    "    res = get_residual(old_cube,new_c,_v)\n",
    "    return(res.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[]': 0, '[0]': 1, '[1]': 0, '[2]': 0, '[0 1]': 1, '[0 2]': 1, '[1 2]': 2, '[0 1 2]': 3}\n",
      "[0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0]\n",
      "[-0.0, -0.5, 0.5, -0.5, 0.5, -0.0, -0.5, -0.0, 0.5, -0.5, 0.5, -0.0]\n",
      "[0.0, 0.5, -0.5, 0.5, -0.5, 0.0, 0.5, 0.0, -0.5, 0.5, -0.5, 0.0]\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0])\n",
      "dict_values([1, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 1])\n",
      "[2.660567633841374e-06, 4.000000891553176, 4.000000259718205]\n",
      "[[nan  1.  0.  0. nan nan nan nan]\n",
      " [-1. nan nan nan  0.  0. nan nan]\n",
      " [ 0. nan nan nan  1. nan  2. nan]\n",
      " [ 0. nan nan nan nan  1.  2. nan]\n",
      " [nan  0. -1. nan nan nan nan  2.]\n",
      " [nan  0. nan -1. nan nan nan  2.]\n",
      " [nan nan -2. -2. nan nan nan  1.]\n",
      " [nan nan nan nan -2. -2. -1. nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import itertools\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "def f_x(x_list):\n",
    "    return 1*x_list[0]+ 2*x_list[1]*x_list[2]\n",
    "pers = [np.array([])] + all_subsets(3)   #所有帶值subset\n",
    "test_value = []\n",
    "a={}\n",
    "for i,per in enumerate(pers):\n",
    "    x_l = [0,0,0]\n",
    "    for j in range(3):\n",
    "        if j in per:\n",
    "            x_l[j] = 1\n",
    "    test_value.append(f_x(x_l))\n",
    "    a[str(per)] = test_value[i]\n",
    "print(a)\n",
    "test_cube = Hypercube(3)\n",
    "test_cube.set_vertex_values(a)\n",
    "from scipy.optimize import minimize \n",
    "x0 = np.array([0.5]*7)\n",
    "f0 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 0,3)[0]\n",
    "f1 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 1,3)[0]\n",
    "f2 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 2,3)[0]\n",
    "\n",
    "    #print('solving first cube...')\n",
    "v0 = minimize(f0, x0)           #最小化殘差\n",
    "    #print('..done')\n",
    "    #print('solving second cube...')\n",
    "v1 = minimize(f1, x0)\n",
    "    #print('..done')\n",
    "\n",
    "    #print('solving third cube...')\n",
    "v2 = minimize(f2, x0)\n",
    "    #print('..done')\n",
    "    \n",
    "# residual = ||▼_feature_cube - ▼cube_feature|| after optimization\n",
    "b = compute_residuals_v(test_cube,v0.x,0,3)\n",
    "gradient_vi = [v0.x, v1.x, v2.x]\n",
    "res_r = []\n",
    "for k  in range(3):\n",
    "    b = compute_residuals_v(test_cube,gradient_vi[k],k,3)\n",
    "    round_b = [ round(k,2) for k in b ]\n",
    "    print(round_b)\n",
    "    res_r.append(sum([abs(r) for r in b]))\n",
    "print(test_cube.partial_gradient[1].values())\n",
    "print(test_cube.E_value.values())\n",
    "print(res_r)\n",
    "test_cube.trans_to_matrix()\n",
    "print(test_cube.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cupy' has no attribute 'optimize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x0 \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Use CuPy's minimize function to find the minimum\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241m.\u001b[39mminimize(objective_function, x0)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimized result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/cupy/__init__.py:927\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _deprecated_apis:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(_numpy, name)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcupy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cupy' has no attribute 'optimize'"
     ]
    }
   ],
   "source": [
    "# Define the objective function (assuming it works with CuPy arrays)\n",
    "def objective_function(x):\n",
    "    # Your objective function implementation goes here\n",
    "    return cp.sum(x ** 2)\n",
    "\n",
    "# Convert the initial guess to a CuPy array\n",
    "x0 = cp.array([0.5] * 7)\n",
    "\n",
    "# Use CuPy's minimize function to find the minimum\n",
    "result = cp.optimize.minimize(objective_function, x0)\n",
    "\n",
    "print(\"Optimized result:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import shap\n",
    "from shap import KernelExplainer # shap套件\n",
    "from sklearn.ensemble import RandomForestClassifier #randomforest\n",
    "#create dataset\n",
    "x1 = np.random.randn(500)\n",
    "x2 = np.random.randn(500)\n",
    "x3 = np.random.randn(500)\n",
    "# label depends on interaction of X1 and X2, and not at all on X3\n",
    "y = np.intp(x1*x2< 1) \n",
    "df = pd.DataFrame({\"Y\":y, \"X1\":x1, \"X2\":x2, \"X3\":x3})\n",
    "features = df.iloc[:,[1,2,3]]\n",
    "labels = df.iloc[:,0]\n",
    "#print(df.head(20))\n",
    "\n",
    "#train model and kernelSHAP\n",
    "# train random forest \n",
    "shapley_value_mean = [0,0,0]\n",
    "resi = [0,0,0]\n",
    "new_resi = [0,0,0]\n",
    "new_n = [[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "model = RandomForestClassifier(n_estimators=25) \n",
    "model.fit(features, labels)  \n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "#測試沒問題，acc94%\n",
    "#acc = 0.\n",
    "#sum = 0.\n",
    "#pred = model.predict(features_test)\n",
    "#for i in range(len(pred)):\n",
    "#    sum += 1\n",
    "#    if pred[i] == labels_test[i]:\n",
    "#        acc += 1\n",
    "#accuracy = acc / sum\n",
    "#print('accuracy = ',accuracy)\n",
    "# train explainer on the model and the data \n",
    "#挑選前50筆資料做為background data，background data的目的是提供base_line等等\n",
    "explainer = KernelExplainer(model.predict_proba,features[:500],link='logit')\n",
    "flag = 1\n",
    "#print(features_test[:5])\n",
    "for i in range(200):\n",
    "    #print('see where')\n",
    "    instance = features.values[i,:]         #features.values把features轉成numpy數組\n",
    "    shap_values = explainer.shap_values(features.values[i,:])\n",
    "#print(shap_values)\n",
    "#shap.summary_plot(shap_values,features_test,show=False)\n",
    "#處理subset\n",
    "    coalition_estimated_values = {str(np.array([])): 0} #建立字典，key是將空numpy array字串化，值都是0\n",
    "    coalitions = [np.array([])] + all_subsets(3)   #所有帶值subset\n",
    "    #print('instance: ',instance)\n",
    "#print('explainer.y: ',explainer.y)\n",
    "   #print(explainer.y[:][:,1])\n",
    "    #print('資料及預測模型平均:',np.mean(labels))\n",
    "    \n",
    "    for coalition in coalitions:                #製作合成資料集，並取得資料集中相同特徵子集的模型預測平均\n",
    "        \n",
    "        synth = pd.DataFrame(explainer.synth_data)\n",
    "        if flag:\n",
    "            flag = 0\n",
    "            synth.to_csv('synth_data.csv', index=False) \n",
    "        for feature in coalition:\n",
    "            synth = synth[synth[feature] == instance[feature]]      #符合相同特徵子集的合成資料\n",
    "            model_mean = np.mean(labels)\n",
    "            impact = np.mean(explainer.y[synth.index][:,1]) - model_mean    #計算符合相同特徵子集的合成資料的標籤平均值減掉原始資料集的baseline      \n",
    "            coalition_estimated_values[str(coalition)] = impact\n",
    "    #製作資料集\n",
    "  \n",
    "        \n",
    "    #synth.to_csv('synth_data.csv', index=False) \n",
    "#print (model.predict_proba([instance]))\n",
    "#print (model.predict_proba([instance])[:,1])\n",
    "#coalition_estimated_values['[0 1 2]'] = np.mean(model.predict_proba([instance])[:,1] - model_mean)\n",
    "   # print('coalition_estimated_values: ',coalition_estimated_values)\n",
    "    cube = Hypercube(3)\n",
    "    cube.set_vertex_values(coalition_estimated_values)\n",
    "    #print(cube.E_value)\n",
    "# constrained optimization: \n",
    "# the null vertex must always have value 0 since it represents the empty coalition,\n",
    "# but all other vertices in the new cube are subject to the minimizer\n",
    "# so x0 has 7 elements instead of 8, and we always append \n",
    "# a 0 to the head of the input array in the optimized functions\n",
    "    from scipy.optimize import minimize \n",
    "    x0 = np.array([0.5]*7)\n",
    "    f0 = lambda x : residual_norm(cube, np.append(np.array(0), x), 0)[0]\n",
    "    f1 = lambda x : residual_norm(cube, np.append(np.array(0), x), 1)[0]\n",
    "    f2 = lambda x : residual_norm(cube, np.append(np.array(0), x), 2)[0]\n",
    "\n",
    "    #print('solving first cube...')\n",
    "    v0 = minimize(f0, x0)           #最小化殘差\n",
    "    #print('..done')\n",
    "    #print('solving second cube...')\n",
    "    v1 = minimize(f1, x0)\n",
    "    #print('..done')\n",
    "\n",
    "    #print('solving third cube...')\n",
    "    v2 = minimize(f2, x0)\n",
    "    #print('..done')\n",
    "  \n",
    "    v = [v0.x,v1.x,v2.x]\n",
    "    #算residuals\n",
    "    shapley_value = []\n",
    "    res_r = []\n",
    "    vi = [v0.x, v1.x, v2.x]\n",
    "    #print(vi)\n",
    "    for j in range(3):\n",
    "        b = compute_residuals_v(cube,vi[j],j)\n",
    "        res_r.append(sum([abs(r) for r in b]))\n",
    "        shapley_value.append(vi[j][-1])\n",
    "        shapley_value_mean[j] +=  abs(shapley_value[j])/200.\n",
    "        resi[j] += res_r[j]/200.\n",
    "    #new_res計算\n",
    "    new_res = [0,0,0]\n",
    "    #print('p_g: ',cube.partial_gradient[0])\n",
    "    for k in range(3):\n",
    "        p_g_e = []\n",
    "        for e in cube.partial_gradient[k]:\n",
    "            if cube.partial_gradient[k][e]!=0:\n",
    "                p_g_e.append(cube.partial_gradient[k][e])\n",
    "        #print('p_g_e:',p_g_e)\n",
    "        p_g_e_n = (p_g_e - np.mean(p_g_e))/np.std(p_g_e)\n",
    "        p_g_e_v = np.std(p_g_e)/np.mean(p_g_e)\n",
    "        new_res[k] = abs(p_g_e_v)\n",
    "        new_resi[k] += new_res[k]\n",
    "        sum_list = [x+abs(y) for x,y in zip(new_n[k],p_g_e_n)]\n",
    "        new_n[k] = sum_list\n",
    "    #print('原始偏移量: ',cube.V_value[\"[0 1 2]\"],'f0偏移: ',v0.x[-1],'f1偏移: ',v1.x[-1],'f2偏移: ',v2.x[-1],'偏移相加: ',v0.x[-1]+v1.x[-1]+v2.x[-1])\n",
    "print('特徵與其餘特徵倆倆關係: ','f1:', new_n[0],'f2:',new_n[1],'f3:',new_n[2])\n",
    "print('特徵的標準差偏移，值越大影響越大: ',new_resi)\n",
    "print('SHapley_residuals: ',resi)\n",
    "print(shapley_value_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_matrix(Hypercube,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[]': 0, '[0]': 1, '[1]': 0, '[2]': 0, '[0 1]': 1, '[0 2]': 1, '[1 2]': 2, '[0 1 2]': 3}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m f2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : residual_norm(test_cube, np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0\u001b[39m), x), \u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('solving first cube...')\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m v0 \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m#最小化殘差\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#print('..done')\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m#print('solving second cube...')\u001b[39;00m\n\u001b[1;32m     37\u001b[0m v1 \u001b[38;5;241m=\u001b[39m minimize(f1, x0)\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    706\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_optimize.py:1477\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1475\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1477\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1481\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_optimize.py:402\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    398\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize \n\u001b[1;32m     28\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.5\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mresidual_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_cube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : residual_norm(test_cube, np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0\u001b[39m), x), \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m f2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : residual_norm(test_cube, np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0\u001b[39m), x), \u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[32], line 107\u001b[0m, in \u001b[0;36mresidual_norm\u001b[0;34m(old_cube, vertex_values, vertex, num_features)\u001b[0m\n\u001b[1;32m    105\u001b[0m new_cube \u001b[38;5;241m=\u001b[39m Hypercube(num_features)\n\u001b[1;32m    106\u001b[0m new_cube\u001b[38;5;241m.\u001b[39mset_vertex_values({\u001b[38;5;28mstr\u001b[39m(_vertex) : vertex_values[j] \u001b[38;5;28;01mfor\u001b[39;00m j, _vertex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(old_cube\u001b[38;5;241m.\u001b[39mV)})   \u001b[38;5;66;03m#將數值設定成0.5\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_cube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_cube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, get_residual(old_cube, new_cube, vertex)\u001b[38;5;241m.\u001b[39mvalues()\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/cupy/_math/sumprod.py:40\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fusion_thread_local\u001b[38;5;241m.\u001b[39mcall_reduction(\n\u001b[1;32m     37\u001b[0m         func, a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# TODO(okuta): check type\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m(axis, dtype, out, keepdims)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import itertools\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "def f_x(x_list):\n",
    "    return 1*x_list[0]+ 2*x_list[1]*x_list[2]\n",
    "pers = [np.array([])] + all_subsets(3)   #所有帶值subset\n",
    "test_value = []\n",
    "a={}\n",
    "for i,per in enumerate(pers):\n",
    "    x_l = [0,0,0]\n",
    "    for j in range(3):\n",
    "        if j in per:\n",
    "            x_l[j] = 1\n",
    "    test_value.append(f_x(x_l))\n",
    "    a[str(per)] = test_value[i]\n",
    "print(a)\n",
    "test_cube = Hypercube(3)\n",
    "test_cube.set_vertex_values(a)\n",
    "from scipy.optimize import minimize \n",
    "x0 = np.array([0.5]*7)\n",
    "f0 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 0,3)[0]\n",
    "f1 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 1,3)[0]\n",
    "f2 = lambda x : residual_norm(test_cube, np.append(np.array(0), x), 2,3)[0]\n",
    "\n",
    "    #print('solving first cube...')\n",
    "v0 = minimize(f0, x0)           #最小化殘差\n",
    "    #print('..done')\n",
    "    #print('solving second cube...')\n",
    "v1 = minimize(f1, x0)\n",
    "    #print('..done')\n",
    "\n",
    "    #print('solving third cube...')\n",
    "v2 = minimize(f2, x0)\n",
    "    #print('..done')\n",
    "test_cube.trans_to_matrix()\n",
    "# residual = ||▼_feature_cube - ▼cube_feature|| after optimization\n",
    "b = compute_residuals_v(test_cube,v0.x,0,3)\n",
    "gradient_vi = [v0.x, v1.x, v2.x]\n",
    "res_r = []\n",
    "for k  in range(3):\n",
    "    b = compute_residuals_v(test_cube,gradient_vi[k],k,3)\n",
    "    round_b = [ round(k,2) for k in b ]\n",
    "    print(round_b)\n",
    "    res_r.append(sum([abs(r) for r in b]))\n",
    "print(test_cube.partial_gradient[1].values())\n",
    "print(test_cube.E_value.values())\n",
    "print(res_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cube = Hypercube(3)\n",
    "cube.set_vertex_values(coalition_estimated_values)\n",
    "print(cube.E_value)\n",
    "# constrained optimization: \n",
    "# the null vertex must always have value 0 since it represents the empty coalition,\n",
    "# but all other vertices in the new cube are subject to the minimizer\n",
    "# so x0 has 7 elements instead of 8, and we always append \n",
    "# a 0 to the head of the input array in the optimized functions\n",
    "from scipy.optimize import minimize \n",
    "x0 = np.array([0.5]*7)\n",
    "f0 = lambda x : residual_norm(cube, np.append(np.array(0), x), 0)\n",
    "f1 = lambda x : residual_norm(cube, np.append(np.array(0), x), 1)\n",
    "f2 = lambda x : residual_norm(cube, np.append(np.array(0), x), 2)\n",
    "\n",
    "print('solving first cube...')\n",
    "v0 = minimize(f0, x0)           #最小化殘差\n",
    "print('..done')\n",
    "print('solving second cube...')\n",
    "v1 = minimize(f1, x0)\n",
    "print('..done')\n",
    "\n",
    "print('solving third cube...')\n",
    "v2 = minimize(f2, x0)\n",
    "print('..done')\n",
    "    \n",
    "# residual = ||▼_feature_cube - ▼cube_feature|| after optimization\n",
    "residuals = [v0.fun, v1.fun, v2.fun]\n",
    "print(residuals)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.array([1, 2, 3, 4, 5])\n",
    "data2 = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "# 计算第一组数据集的标准差\n",
    "std1 = np.std(data1)/np.mean(data1)\n",
    "\n",
    "# 计算第二组数据集的标准差\n",
    "std2 = np.std(data2)/np.mean(data2)\n",
    "\n",
    "print(std1,std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,1,1]\n",
    "c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "coalition_estimated_values = {str(np.array([])): 0} #建立字典，key是將空numpy array字串化，值都是0\n",
    "coalitions = [np.array([])] + all_subsets(3)   #所有帶值subset\n",
    "instance = [0,1,2]\n",
    "dataset = []\n",
    "num = 100\n",
    "for coalition in coalitions:\n",
    "    s = len(coalition)**4 + 1\n",
    "    create_data = []\n",
    "    for i in range(int(100/s)):\n",
    "        create_data.append([random.random(),random.random(),random.random()])\n",
    "    for feature in coalition:\n",
    "        for data in create_data:\n",
    "            data[feature] = instance[feature] \n",
    "    dataset += create_data\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process import read_selected_data, get_y,  split_data, compute_class_weights\n",
    "from dataset import BertDataset\n",
    "from model import BertClassifier\n",
    "from training import train_model\n",
    "from utils import draw_pics, initial_record\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "total_params = 14\n",
    "csv_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/processed_data/v014_stage_1.csv'\n",
    "json_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/controllable_para_v014_14.json'\n",
    "tool_name = 'ASCVD'\n",
    "epochs = 50000\n",
    "lr = 1e-5\n",
    "batch_size = 1024\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "save_folder_name = 'stage-1-param_'+str(total_params)+'-batch_'+str(batch_size)+'-lr_'+str(lr)\n",
    "with open(json_file_path, 'r') as f:\n",
    "    params = json.load(f)\n",
    "    f.close()\n",
    "s1_df = pd.read_csv(csv_file_path)\n",
    "#s1_df.shape\n",
    "all_key = list(params[tool_name]) \n",
    "params_list = []                #取得json檔內的特徵\n",
    "for key in all_key:\n",
    "    all_param = params[tool_name][key]\n",
    "    if(type(all_param) == list):\n",
    "        for param in all_param:\n",
    "             params_list.append(param)\n",
    "    else:\n",
    "        params_list.append(all_param)\n",
    "    \n",
    "# 取得Json檔內包含的特徵\n",
    "s1_df = s1_df[params_list] \n",
    "#print(params)\n",
    "s1_df.head(10)\n",
    "feature_df = s1_df.drop(['DFT_CNT'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchmin (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchmin\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 8, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(feature_df)\n",
    "nf_df = pd.DataFrame(X_standardized)\n",
    "nf_df\n",
    "param_group = [] # [2,2,4,2]\n",
    "all_key = list(params[tool_name]) # ['EQ', 'PUMP', 'CH', 'VENT', 'y']\n",
    "all_key.remove('y')\n",
    "\n",
    "for key in all_key:\n",
    "    all_value = params[tool_name][key]\n",
    "    param_group.append(len(all_value))\n",
    "param_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_zero(df, tool_name, total_params, flag, params=params): \n",
    "    # 將一維參數matrix擴展為4維\n",
    "    data_arr = df.to_numpy()\n",
    "    result = []\n",
    "    for i in range(len(data_arr)):\n",
    "        arr_index = 0\n",
    "        empty_arr = np.zeros((4,total_params)) # chamber數 * 總參數數量\n",
    "        param_group_cp = param_group.copy()\n",
    "        for j in range(len(empty_arr)):\n",
    "            while(param_group_cp[j] > 0):\n",
    "                empty_arr[j][arr_index] = data_arr[i][arr_index]\n",
    "                param_group_cp[j] -= 1\n",
    "                arr_index += 1\n",
    "        \n",
    "        if(flag == 1): # bert.py使用\n",
    "            result.append(empty_arr)\n",
    "        if(flag == 2): # bert_du.py使用\n",
    "            result.append(empty_arr.tolist())\n",
    "    \n",
    "    if(flag == 1): # bert.py使用\n",
    "        result = pd.DataFrame({'X': [result[i] for i in range(len(result))]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_df_4d = padding_zero(nf_df,tool_name,total_params,flag=1)\n",
    "nf_df_4d_object = nf_df_4d.to_numpy()\n",
    "nf_df_4d_list = []\n",
    "for i in range(len(nf_df_4d_object)):\n",
    "    nf_df_4d_list.append(nf_df_4d_object[i][0])\n",
    "nf_df_4d_arr = np.array(nf_df_4d_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料模型預測及平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "s1_model_path = '/hcds_vol/private/luffy/GANGAN-master/model/predictor/stage_1_checkpoint.pth'\n",
    "s1_model =  torch.load(s1_model_path).to(device)\n",
    "s1_model.eval()\n",
    "nf_df_4d_tensor = torch.tensor(nf_df_4d_arr,dtype=torch.float)\n",
    "dataset = TensorDataset(nf_df_4d_tensor)\n",
    "batch_size = 256\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "        batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "        batch_output = s1_model(batch_data)\n",
    "        probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "        outputs += probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "取得模型平均和對應output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_arr = np.array([output.cpu().numpy()[0] for output in outputs])\n",
    "output_df = pd.DataFrame({'Output': output_arr})\n",
    "new_df = pd.concat([feature_df,output_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import itertools\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "AUO_coalitions = [np.array([])] + all_subsets(14) \n",
    "coalition_estimated_values = {}\n",
    "instance = new_df.iloc[0]\n",
    "synth = new_df \n",
    "mean_exp = synth['Output'].mean()\n",
    "for coalition in AUO_coalitions:\n",
    "    for feature in coalition:\n",
    "       synth = synth[synth[params_list[feature]]==instance[params_list[feature]]]\n",
    "    impact = synth['Output'].mean() - mean_exp\n",
    "    coalition_estimated_values[str(coalition)] = impact\n",
    "    print(impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUOcube = Hypercube(14)\n",
    "print(coalition_estimated_values)\n",
    "AUOcube.set_vertex_values(coalition_estimated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 使用的 CUDA 版本: 12.1\n",
      "Collecting cupy-cuda12x\n",
      "  Downloading cupy_cuda12x-13.0.0-cp39-cp39-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy<1.29,>=1.22 in /hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages (from cupy-cuda12x) (1.26.4)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x)\n",
      "  Downloading fastrlock-0.8.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Downloading cupy_cuda12x-13.0.0-cp39-cp39-manylinux2014_x86_64.whl (89.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastrlock, cupy-cuda12x\n",
      "Successfully installed cupy-cuda12x-13.0.0 fastrlock-0.8.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 使用的 CUDA 版本:\", torch.version.cuda)\n",
    "!pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUOcube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = []\n",
    "c = [1,2,3]\n",
    "if a==c or b==c:\n",
    "    print(1)\n",
    "else: \n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(10000, 10000)\n",
    "b = np.random.rand(10000)\n",
    "c = b/A\n",
    "print(c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
