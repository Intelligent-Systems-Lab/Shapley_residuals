{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import bisect\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "class Hypercube:                #超立方體\n",
    "    '''\n",
    "    A class to create a hypercube object which stores values on vertices\n",
    "    and values on the edges between neighboring vertices\n",
    "    '''    \n",
    "    #輸入維度、(點鍵值)、(點值)\n",
    "    def __init__(self, n_vertices, vertex_keys = None, vertex_values = None):   \n",
    "        self.n_vertices = n_vertices\n",
    "        self.v_num = 2**self.n_vertices\n",
    "        self.V = [np.array([])] + all_subsets(n_vertices)   #所有子集包含空集，即所有點\n",
    "        self.V_value = {str(v) : 0. for v in self.V}         #所有點值  先設為0\n",
    "        self.E = []\n",
    "        self.E_value = {}\n",
    "        self.partial_gradient = {vertex : {} for vertex in range(n_vertices)}   #各個維度的partial gradient\n",
    "        self.matrix = np.full((self.v_num,self.v_num),np.nan)                   #原始立方體對應到的矩陣\n",
    "        self.partial_gradient_matrix = np.full((self.v_num,self.v_num),np.nan)  #對特定 feature的partial gradient\n",
    "        self.vi_matrix = np.full((self.v_num,self.v_num),np.nan)                #v_i對應到的矩陣\n",
    "        self.res_matrix = np.full((self.v_num,self.v_num),np.nan)               #residual的matrix\n",
    "        self.sv = 0\n",
    " \n",
    "    def set_vertex_values(self, vertex_values):         #設置點值\n",
    "        for v in vertex_values:                         #用鍵值來做查找\n",
    "            self.V_value[v] = vertex_values[v]\n",
    "            \n",
    "        # edge values are the differences between neighboring vertex values\n",
    "        #self._calculate_edges()\n",
    "        \n",
    "    def _calculate_edges(self):                 #計算邊值\n",
    "        \n",
    "        # calculate the usual gradients: the difference between neighboring edges\n",
    "        for i, v in enumerate(self.V):\n",
    "            for _v in self.V[i+1:]:\n",
    "                if self._vertices_form_a_valid_edge(v, _v):\n",
    "                    self.E.append((v, _v))\n",
    "                    self.E_value[str((v, _v))] = self.V_value[str(_v)] - self.V_value[str(v)]\n",
    "        \n",
    "        # calculate partial gradients\n",
    "        for vertex in range(self.n_vertices):\n",
    "            self.partial_gradient[vertex] = self.E_value.copy()\n",
    "            for v, _v in self.E:\n",
    "                is_relevant_edge_for_partial_gradient = (vertex in v and vertex not in _v) or (vertex in _v and vertex not in v)\n",
    "                if not is_relevant_edge_for_partial_gradient:\n",
    "                    self.partial_gradient[vertex][str((v, _v))] = 0.\n",
    "            \n",
    "    def _vertices_form_a_valid_edge(self, v, _v):       #檢查交集和是否相鄰\n",
    "        # vertices are neighbors in a hypercube\n",
    "        # if they differ by exactly one element\n",
    "        differ_in_size_by_1 = (abs(len(v) - len(_v)) == 1)      #差距是1\n",
    "    \n",
    "        the_intersection = np.intersect1d(v, _v)                #兩個集合的交集\n",
    "        #print(type(v[0]),type(_v[0]),type(the_intersection[0]))\n",
    "        intersection_is_nonempty = len(the_intersection) > 0 or len(v)==0 or len(_v) == 0   #交集大於0或v,_v是空集\n",
    "        is_intersection = False                         \n",
    "        if len(the_intersection)>0:                              \n",
    "            if len(the_intersection)==len(v) or len(the_intersection)==len(_v):\n",
    "                is_intersection = True\n",
    "        else:\n",
    "            if len(v)==0 and len(_v)==1:\n",
    "                is_intersection = True\n",
    "       # print(is_intersection)\n",
    "        return differ_in_size_by_1 and intersection_is_nonempty and is_intersection\n",
    "    \n",
    "    #create matrix for Hypercube\n",
    "    def trans_to_matrix(self,feature_i):                #超立方體轉換成矩陣\n",
    "        for i, v in enumerate(self.V):                  #對立方體上所有點\n",
    "            for j,_v in enumerate(self.V[i+1:]):        \n",
    "                if self._vertices_form_a_valid_edge(v, _v):     #是否相交\n",
    "                    self.matrix[i][i+j+1] = self.V_value[str(_v)] - self.V_value[str(v)]    #獲得matrix\n",
    "                    self.matrix[i+j+1][i] = self.V_value[str(v)] - self.V_value[str(_v)]\n",
    "        \n",
    "            \n",
    "        self.partial_gradient_matrix = self.matrix.copy()       #把原始立方體複製下來，保留i方向\n",
    "        self.my_list = []\n",
    "        self.partial_gradient_vector = np.array([])\n",
    "        self.weight_vector = np.array([])\n",
    "        self.partial_gradient_norm = 0\n",
    "        for j, v in enumerate(self.V):\n",
    "            for k,_v in enumerate(self.V[j+1:]):\n",
    "                if self._vertices_form_a_valid_edge(v, _v):     #如果是邊\n",
    "                    is_relevant_edge_for_partial_gradient = (feature_i in v and feature_i not in _v) or (feature_i in _v and feature_i not in v)\n",
    "                    if not is_relevant_edge_for_partial_gradient:           #如果 特徵i不在v 或_v，把那條邊設成0\n",
    "                        self.partial_gradient_matrix[j][j+k+1] = 0.\n",
    "                        self.partial_gradient_matrix[j+k+1][j] = 0.\n",
    "                    else:\n",
    "                        self.partial_gradient_norm += (self.partial_gradient_matrix[j][j+k+1])**2       # norm\n",
    "                        self.partial_gradient_vector = np.append(self.partial_gradient_vector,self.partial_gradient_matrix[j][j+k+1])      #比較向量1\n",
    "                        subset_len = len(v)\n",
    "                        weight = 1./(math.comb(self.n_vertices,subset_len)*(self.n_vertices-subset_len))            #Shapley value權重參數\n",
    "                        self.weight_vector = np.append(self.weight_vector,weight)\n",
    "                        self.my_list.append((j,j+k+1))          \n",
    "        #self.vi[feature_i] = self.shapley_residuals_in_matrix()\n",
    "        #self.vi[i] = self.shapley_residuals_in_matrix\n",
    "    #minimize the function (gradient_x - partial_gradient_i)^2 最小化l2_norm\n",
    "    def shapley_residuals_in_matrix(self):\n",
    "            derivative_i  = np.full((self.v_num,self.v_num),0.)      #計算微分後得到的方程式矩陣 A\n",
    "            b_i = np.array([0.]*self.v_num)                          #得到Ax = b 的 b向量值\n",
    "            for j  in range(self.v_num):                            #對矩陣的每個點\n",
    "                for k in range(self.v_num):\n",
    "                    if np.isnan(self.partial_gradient_matrix[j][k]):    #不用計算nan\n",
    "                        continue\n",
    "                    #elif j == 0 or k ==0:                               #如果是跟原點相鄰\n",
    "                     #   derivative_i[j][j] += 1.                       #係數+1\n",
    "                      #  b_i[j] += - self.partial_gradient_matrix[j][k]     #x_j-x_i-partial_gradient\n",
    "                    else:                                               \n",
    "                        derivative_i[j][j] += 1.                         \n",
    "                        derivative_i[j][k] += -1.\n",
    "                        b_i[j] += - self.partial_gradient_matrix[j][k]\n",
    "            A = derivative_i[1:,1:]                                    #只要x_i for i!=0\n",
    "            \n",
    "            B = b_i[1:]                                                #保留b_i\n",
    "            res = 0.\n",
    "            A_inverse = np.linalg.inv(A)                               #算inverse matrix\n",
    "            vi = np.insert(np.dot(A_inverse,B),0,0.)                    #vi  = b/A #開頭補0\n",
    "            vi_V =  [np.array([])] + all_subsets(self.n_vertices)\n",
    "            vi_V_value = {str(v) : 0. for v in vi_V} \n",
    "            res_dic = {}\n",
    "            for k,v in enumerate(vi_V):               \n",
    "                vi_V_value[str(v)] = vi[k]\n",
    "            self.vi_vector = np.array([])                              #比較向量#2\n",
    "            for i, v in enumerate(vi_V):\n",
    "                for j,_v in enumerate(vi_V[i+1:]):\n",
    "                    if self._vertices_form_a_valid_edge(v, _v):\n",
    "                        self.vi_matrix[i][i+j+1] = vi_V_value[str(_v)] - vi_V_value[str(v)]\n",
    "                        self.vi_matrix[i+j+1][i] = vi_V_value[str(v)] - vi_V_value[str(_v)]\n",
    "                        self.res_matrix[i][i+j+1] = self.partial_gradient_matrix[i][i+j+1] - self.vi_matrix[i][i+j+1]\n",
    "                        self.res_matrix[i+j+1][i] = self.partial_gradient_matrix[i+j+1][i] - self.vi_matrix[i+j+1][i]\n",
    "                        res += (self.res_matrix[i][i+j+1])**2\n",
    "                        res_dic[str(v)+'->'+str(_v)] = self.res_matrix[i][i+j+1]\n",
    "                        if (i,i+j+1) in self.my_list:\n",
    "                            self.vi_vector = np.append(self.vi_vector,self.vi_matrix[i][i+j+1])     #比較向量#2\n",
    "            #print(self.my_list)\n",
    "            #print(self.partial_gradient_vector,self.vi_vector)\n",
    "            vector_A = self.weight_vector*self.partial_gradient_vector\n",
    "            vector_B = self.weight_vector*self.vi_vector\n",
    "            cos_sim = self.cos_sim(vector_A,vector_B)\n",
    "            print('cos_sim = ',cos_sim)\n",
    "            self.sv += vi[-1]\n",
    "            print('shapley_value: ',vi[-1],'residual sum: ',res)\n",
    "            #print(self.sv)\n",
    "            #print('residuals_sum:',res,'shapley_value: ',vi)\n",
    "            res =  (res/self.partial_gradient_norm)**0.5\n",
    "            print('norm: ',res)\n",
    "            \n",
    "            return vi_V_value, res_dic, self.partial_gradient_norm\n",
    "    \n",
    "    def cos_sim(self,a,b):\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        similarity = dot_product / (norm_a * norm_b)\n",
    "        return similarity\n",
    "            \n",
    "####################     \n",
    "def save_json(dic,feature_name,folder_path):\n",
    "    filename = f\"dic_{feature_name}.json\"\n",
    "    file_path = os.path.join(folder_path,filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(dic, json_file)\n",
    "\n",
    "def save_res_json(dic,feature_name,folder_path):\n",
    "    filename = f\"dic_{feature_name}_res.json\"\n",
    "    file_path = os.path.join(folder_path,filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(dic, json_file)\n",
    "\n",
    "        \n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "\n",
    "def get_residual(old_cube, new_cube, vertex):   #計算殘差\n",
    "    '''\n",
    "    returns: residual dictionary\n",
    "        \n",
    "        { edge : ▼_player_v[edge] - ▼v_player[edge] for edge in old_cube }\n",
    "    '''\n",
    "    assert set(old_cube.E_value.keys()) == set(new_cube.E_value.keys())     #判斷兩個字典中鍵值的組合是否相同。assert:\n",
    "    res = {}\n",
    "    for e in old_cube.E_value.keys():\n",
    "        res[e] = old_cube.partial_gradient[vertex][e] - new_cube.E_value[e] #對應某特徵的邊相減 即gradient_i_v - gradient_v_i(殘差)\n",
    "    return res\n",
    "count = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "#記得添加number of vertex\n",
    "def residual_norm(old_cube, vertex_values, vertex,num_features):     #old_cube是原本的SHAP得到的立方體\n",
    "    '''\n",
    "    old_cube: v, our game\n",
    "    vertex: player\n",
    "    vertex_values: v_player, proposed game\n",
    "    \n",
    "    assumes that the order of the values in vertex_values align with the order of the values in old_cube.V\n",
    "    \n",
    "    returns: || ▼_player_v - ▼v_player ||\n",
    "    '''\n",
    "    if count[vertex]==0 :\n",
    "        count[vertex] += 1\n",
    "    new_cube = Hypercube(num_features)\n",
    "    new_cube.set_vertex_values({str(_vertex) : vertex_values[j] for j, _vertex in enumerate(old_cube.V)})   #將數值設定成0.5\n",
    "    return np.sum([(r)**2 for r in get_residual(old_cube, new_cube, vertex).values()]), get_residual(old_cube, new_cube, vertex).values() #計算所有residual造成的影響加總\n",
    "#改一下參數\n",
    "def compute_residuals_v(old_cube,vertex_of_v_i_cube,_v,num_features):            #(instance cube,算出來的v_i cube,這個cube的指定feature)\n",
    "    new_vertex =  np.append(np.array(0), vertex_of_v_i_cube)\n",
    "    new_c = Hypercube(num_features)\n",
    "    coalitions = [np.array([])] + all_subsets(num_features)\n",
    "    b = {}\n",
    "    for i, coalition in enumerate(coalitions):\n",
    "        b[str(coalition)] = new_vertex[i]\n",
    "    new_c.set_vertex_values(b)\n",
    "    res = get_residual(old_cube,new_c,_v)\n",
    "    return(res.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from itertools import combinations\\n\\ndef generate_all_subsets(num):\\n    num_set = [i for i in range(num)]\\n    return [np.array(s) for r in range(num+1) for s in combinations(num_set, r) ]\\ndef all_subsets(n_elts):\\n        #returns a list of 2^{n_elts} lists\\n        #each a different subset of {1, 2,...,n_elts}\\n    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\\n    res = {i : res[i] for i in range(n_elts)}\\n    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\\n    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\\na  = generate_all_subsets(5)\\n\\nb = [np.array([])]+all_subsets(5)\\nprint(a)\\nprint(b)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from itertools import combinations\n",
    "\n",
    "def generate_all_subsets(num):\n",
    "    num_set = [i for i in range(num)]\n",
    "    return [np.array(s) for r in range(num+1) for s in combinations(num_set, r) ]\n",
    "def all_subsets(n_elts):\n",
    "        #returns a list of 2^{n_elts} lists\n",
    "        #each a different subset of {1, 2,...,n_elts}\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "a  = generate_all_subsets(5)\n",
    "\n",
    "b = [np.array([])]+all_subsets(5)\n",
    "print(a)\n",
    "print(b)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from data_process import read_selected_data, get_y,  split_data, compute_class_weights\n",
    "from dataset import BertDataset\n",
    "from model import BertClassifier\n",
    "from training import train_model\n",
    "from utils import draw_pics, initial_record\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "total_params = 14\n",
    "csv_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/processed_data/v014_stage_1.csv'\n",
    "json_file_path = '/hcds_vol/private/luffy/GANGAN-master/data/controllable_para_v014_14.json'\n",
    "tool_name = 'ASCVD'\n",
    "epochs = 50000\n",
    "lr = 1e-5\n",
    "batch_size = 1024\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "save_folder_name = 'stage-1-param_'+str(total_params)+'-batch_'+str(batch_size)+'-lr_'+str(lr)\n",
    "with open(json_file_path, 'r') as f:\n",
    "    params = json.load(f)\n",
    "    f.close()\n",
    "s1_df = pd.read_csv(csv_file_path)\n",
    "#s1_df.shape\n",
    "all_key = list(params[tool_name]) \n",
    "params_list = []                #取得json檔內的特徵\n",
    "for key in all_key:\n",
    "    all_param = params[tool_name][key]\n",
    "    if(type(all_param) == list):\n",
    "        for param in all_param:\n",
    "             params_list.append(param)\n",
    "    else:\n",
    "        params_list.append(all_param)\n",
    "    \n",
    "# 取得Json檔內包含的特徵\n",
    "s1_df = s1_df[params_list] \n",
    "#print(params)\n",
    "s1_df.head(10)\n",
    "feature_df = s1_df.drop(['DFT_CNT'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 8, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(feature_df)\n",
    "nf_df = pd.DataFrame(X_standardized)\n",
    "nf_df\n",
    "param_group = [] # [2,2,4,2]\n",
    "all_key = list(params[tool_name]) # ['EQ', 'PUMP', 'CH', 'VENT', 'y']\n",
    "all_key.remove('y')\n",
    "\n",
    "for key in all_key:\n",
    "    all_value = params[tool_name][key]\n",
    "    param_group.append(len(all_value))\n",
    "param_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_zero(df, tool_name, total_params, flag, params=params): \n",
    "    # 將一維參數matrix擴展為4維\n",
    "    data_arr = df.to_numpy()\n",
    "    result = []\n",
    "    for i in range(len(data_arr)):\n",
    "        arr_index = 0\n",
    "        empty_arr = np.zeros((4,total_params)) # chamber數 * 總參數數量\n",
    "        param_group_cp = param_group.copy()\n",
    "        for j in range(len(empty_arr)):\n",
    "            while(param_group_cp[j] > 0):\n",
    "                empty_arr[j][arr_index] = data_arr[i][arr_index]\n",
    "                param_group_cp[j] -= 1\n",
    "                arr_index += 1\n",
    "        \n",
    "        if(flag == 1): # bert.py使用\n",
    "            result.append(empty_arr)\n",
    "        if(flag == 2): # bert_du.py使用\n",
    "            result.append(empty_arr.tolist())\n",
    "    \n",
    "    if(flag == 1): # bert.py使用\n",
    "        result = pd.DataFrame({'X': [result[i] for i in range(len(result))]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_df_4d = padding_zero(nf_df,tool_name,total_params,flag=1)\n",
    "nf_df_4d_object = nf_df_4d.to_numpy()\n",
    "nf_df_4d_list = []\n",
    "for i in range(len(nf_df_4d_object)):\n",
    "    nf_df_4d_list.append(nf_df_4d_object[i][0])\n",
    "nf_df_4d_arr = np.array(nf_df_4d_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "友達資料模型預測及平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "s1_model_path = '/hcds_vol/private/luffy/GANGAN-master/model/predictor/stage_1_checkpoint.pth'\n",
    "s1_model =  torch.load(s1_model_path).to(device)\n",
    "s1_model.eval()\n",
    "nf_df_4d_tensor = torch.tensor(nf_df_4d_arr,dtype=torch.float)\n",
    "dataset = TensorDataset(nf_df_4d_tensor)\n",
    "batch_size = 256\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "        batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "        batch_output = s1_model(batch_data)\n",
    "        probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "        outputs += probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得模型平均和對應output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_-TACT_TIME_mean</th>\n",
       "      <th>X_-CONVEYOR_SPEED_mean</th>\n",
       "      <th>PUMP_high</th>\n",
       "      <th>PUMP_low</th>\n",
       "      <th>CLN1_over-etching-ratio</th>\n",
       "      <th>CLN1_EPT_time</th>\n",
       "      <th>clean_count</th>\n",
       "      <th>EPT_clean_count_ratio</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-max</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-range</th>\n",
       "      <th>NH3_TREAT_-RF_FREQ-mean</th>\n",
       "      <th>NP_3_-MFC_VOL_SIH4-range</th>\n",
       "      <th>VENT_high</th>\n",
       "      <th>VENT_low</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>3150</td>\n",
       "      <td>29848.68000</td>\n",
       "      <td>10349.70000</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>9987</td>\n",
       "      <td>3</td>\n",
       "      <td>3329.000</td>\n",
       "      <td>13947</td>\n",
       "      <td>444</td>\n",
       "      <td>13614.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>14870.22222</td>\n",
       "      <td>5360.350000</td>\n",
       "      <td>0.922084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>4200</td>\n",
       "      <td>37183.04000</td>\n",
       "      <td>12891.48000</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>10023</td>\n",
       "      <td>2</td>\n",
       "      <td>5011.500</td>\n",
       "      <td>13963</td>\n",
       "      <td>442</td>\n",
       "      <td>13623.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>16217.42000</td>\n",
       "      <td>8937.611111</td>\n",
       "      <td>0.672468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>4200</td>\n",
       "      <td>40499.96000</td>\n",
       "      <td>12198.94000</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>10059</td>\n",
       "      <td>3</td>\n",
       "      <td>3353.000</td>\n",
       "      <td>13948</td>\n",
       "      <td>418</td>\n",
       "      <td>13649.4286</td>\n",
       "      <td>1</td>\n",
       "      <td>17061.03750</td>\n",
       "      <td>5707.557143</td>\n",
       "      <td>0.576102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>4200</td>\n",
       "      <td>35524.30000</td>\n",
       "      <td>12731.66000</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>9999</td>\n",
       "      <td>3</td>\n",
       "      <td>3333.000</td>\n",
       "      <td>13979</td>\n",
       "      <td>451</td>\n",
       "      <td>13610.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>18121.24667</td>\n",
       "      <td>6610.381818</td>\n",
       "      <td>0.739274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>4200</td>\n",
       "      <td>40188.72000</td>\n",
       "      <td>10520.93333</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>9996</td>\n",
       "      <td>7</td>\n",
       "      <td>1428.000</td>\n",
       "      <td>13967</td>\n",
       "      <td>433</td>\n",
       "      <td>13642.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>16771.88824</td>\n",
       "      <td>9810.666667</td>\n",
       "      <td>0.859217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40971</th>\n",
       "      <td>140</td>\n",
       "      <td>3150</td>\n",
       "      <td>53015.43333</td>\n",
       "      <td>10077.01667</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>12069</td>\n",
       "      <td>8</td>\n",
       "      <td>1508.625</td>\n",
       "      <td>13994</td>\n",
       "      <td>468</td>\n",
       "      <td>13643.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>17537.10000</td>\n",
       "      <td>6312.560000</td>\n",
       "      <td>0.847003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40972</th>\n",
       "      <td>140</td>\n",
       "      <td>3150</td>\n",
       "      <td>53015.43333</td>\n",
       "      <td>12465.30000</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>12021</td>\n",
       "      <td>2</td>\n",
       "      <td>6010.500</td>\n",
       "      <td>13967</td>\n",
       "      <td>441</td>\n",
       "      <td>13636.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>16070.72500</td>\n",
       "      <td>7336.800000</td>\n",
       "      <td>0.829222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40973</th>\n",
       "      <td>140</td>\n",
       "      <td>3150</td>\n",
       "      <td>58385.13333</td>\n",
       "      <td>13051.28571</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>11994</td>\n",
       "      <td>3</td>\n",
       "      <td>3998.000</td>\n",
       "      <td>13975</td>\n",
       "      <td>450</td>\n",
       "      <td>13639.0833</td>\n",
       "      <td>1</td>\n",
       "      <td>16219.02500</td>\n",
       "      <td>6658.808333</td>\n",
       "      <td>0.837974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40974</th>\n",
       "      <td>140</td>\n",
       "      <td>3150</td>\n",
       "      <td>73952.03333</td>\n",
       "      <td>12358.76000</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>12021</td>\n",
       "      <td>3</td>\n",
       "      <td>4007.000</td>\n",
       "      <td>13979</td>\n",
       "      <td>448</td>\n",
       "      <td>13612.4545</td>\n",
       "      <td>1</td>\n",
       "      <td>18019.51429</td>\n",
       "      <td>7409.445455</td>\n",
       "      <td>0.826851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40975</th>\n",
       "      <td>140</td>\n",
       "      <td>3150</td>\n",
       "      <td>62161.70000</td>\n",
       "      <td>13165.42857</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>11994</td>\n",
       "      <td>4</td>\n",
       "      <td>2998.500</td>\n",
       "      <td>13968</td>\n",
       "      <td>446</td>\n",
       "      <td>13651.1429</td>\n",
       "      <td>1</td>\n",
       "      <td>16464.53125</td>\n",
       "      <td>7022.027273</td>\n",
       "      <td>0.870864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40976 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_-TACT_TIME_mean  X_-CONVEYOR_SPEED_mean    PUMP_high     PUMP_low  \\\n",
       "0                     90                    3150  29848.68000  10349.70000   \n",
       "1                     90                    4200  37183.04000  12891.48000   \n",
       "2                     90                    4200  40499.96000  12198.94000   \n",
       "3                     90                    4200  35524.30000  12731.66000   \n",
       "4                     90                    4200  40188.72000  10520.93333   \n",
       "...                  ...                     ...          ...          ...   \n",
       "40971                140                    3150  53015.43333  10077.01667   \n",
       "40972                140                    3150  53015.43333  12465.30000   \n",
       "40973                140                    3150  58385.13333  13051.28571   \n",
       "40974                140                    3150  73952.03333  12358.76000   \n",
       "40975                140                    3150  62161.70000  13165.42857   \n",
       "\n",
       "       CLN1_over-etching-ratio  CLN1_EPT_time  clean_count  \\\n",
       "0                     0.001802           9987            3   \n",
       "1                     0.003292          10023            2   \n",
       "2                     0.005368          10059            3   \n",
       "3                     0.008401           9999            3   \n",
       "4                     0.017107           9996            7   \n",
       "...                        ...            ...          ...   \n",
       "40971                 0.001491          12069            8   \n",
       "40972                 0.006489          12021            2   \n",
       "40973                 0.001251          11994            3   \n",
       "40974                 0.006489          12021            3   \n",
       "40975                 0.001251          11994            4   \n",
       "\n",
       "       EPT_clean_count_ratio  NH3_TREAT_-RF_FREQ-max  \\\n",
       "0                   3329.000                   13947   \n",
       "1                   5011.500                   13963   \n",
       "2                   3353.000                   13948   \n",
       "3                   3333.000                   13979   \n",
       "4                   1428.000                   13967   \n",
       "...                      ...                     ...   \n",
       "40971               1508.625                   13994   \n",
       "40972               6010.500                   13967   \n",
       "40973               3998.000                   13975   \n",
       "40974               4007.000                   13979   \n",
       "40975               2998.500                   13968   \n",
       "\n",
       "       NH3_TREAT_-RF_FREQ-range  NH3_TREAT_-RF_FREQ-mean  \\\n",
       "0                           444               13614.0000   \n",
       "1                           442               13623.0000   \n",
       "2                           418               13649.4286   \n",
       "3                           451               13610.0000   \n",
       "4                           433               13642.2500   \n",
       "...                         ...                      ...   \n",
       "40971                       468               13643.0000   \n",
       "40972                       441               13636.2500   \n",
       "40973                       450               13639.0833   \n",
       "40974                       448               13612.4545   \n",
       "40975                       446               13651.1429   \n",
       "\n",
       "       NP_3_-MFC_VOL_SIH4-range    VENT_high     VENT_low    Output  \n",
       "0                             0  14870.22222  5360.350000  0.922084  \n",
       "1                             1  16217.42000  8937.611111  0.672468  \n",
       "2                             1  17061.03750  5707.557143  0.576102  \n",
       "3                             1  18121.24667  6610.381818  0.739274  \n",
       "4                             1  16771.88824  9810.666667  0.859217  \n",
       "...                         ...          ...          ...       ...  \n",
       "40971                         1  17537.10000  6312.560000  0.847003  \n",
       "40972                         1  16070.72500  7336.800000  0.829222  \n",
       "40973                         1  16219.02500  6658.808333  0.837974  \n",
       "40974                         1  18019.51429  7409.445455  0.826851  \n",
       "40975                         1  16464.53125  7022.027273  0.870864  \n",
       "\n",
       "[40976 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output_arr = np.array([output.cpu().numpy()[0] for output in outputs])\n",
    "output_df = pd.DataFrame({'Output': output_arr})\n",
    "new_df = pd.concat([feature_df,output_df],axis=1)\n",
    "new_df.to_csv('data_with_output.csv',index=False)\n",
    "new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(data):\n",
    "    if 'Output' in data.columns:\n",
    "        data = data.drop(columns=['Output'])\n",
    "    data_standardized = scaler.transform(data)\n",
    "    data_standardized_df = pd.DataFrame(data_standardized)\n",
    "    data_4d = padding_zero(data_standardized_df,tool_name,total_params,flag=1)\n",
    "    data_4d_object =  data_4d.to_numpy()\n",
    "    data_4d_list = []\n",
    "    for i in range(len(data_4d_object)):\n",
    "        data_4d_list.append(data_4d_object[i][0])\n",
    "    data_4d_arr = np.array(data_4d_list)\n",
    "    data_4d_tensor = torch.tensor(data_4d_arr,dtype=torch.float)\n",
    "    my_dataset = TensorDataset(data_4d_tensor)\n",
    "    batch_size = 256\n",
    "    my_loader = DataLoader(my_dataset, batch_size=batch_size,num_workers=4)\n",
    "    data_output = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in my_loader:\n",
    "        # 将数据移到指定的设备上（如 CUDA 设备）\n",
    "            batch_data = batch_data[0].to(device)\n",
    "        \n",
    "        # 将数据传递给模型进行推理\n",
    "            batch_output = s1_model(batch_data)\n",
    "            probs = (torch.nn.functional.softmax(batch_output, dim=1))\n",
    "        # 将输出保存起来\n",
    "            data_output += probs\n",
    "    data_output_arr = np.array([output.cpu().numpy()[0] for output in data_output])\n",
    "    data_expectation_out = data_output_arr.mean()\n",
    "    return data_expectation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_-TACT_TIME_mean', 'X_-CONVEYOR_SPEED_mean', 'PUMP_high', 'PUMP_low', 'CLN1_over-etching-ratio', 'CLN1_EPT_time', 'clean_count', 'EPT_clean_count_ratio', 'NH3_TREAT_-RF_FREQ-max', 'NH3_TREAT_-RF_FREQ-range', 'NH3_TREAT_-RF_FREQ-mean', 'NP_3_-MFC_VOL_SIH4-range', 'VENT_high', 'VENT_low', 'DFT_CNT']\n"
     ]
    }
   ],
   "source": [
    "print(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31661\n",
      "0.85921710729599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#if (2 in coalition and 3 not in coalition):\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#    PUMP_high = instance.iloc[2]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#    synth = synth[synth.iloc[:,3]<PUMP_high]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#print(coalition)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#count_n = 0\u001b[39;00m\n\u001b[1;32m     58\u001b[0m count_n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 59\u001b[0m Exp \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m impact \u001b[38;5;241m=\u001b[39m Exp \u001b[38;5;241m-\u001b[39m mean_exp\n\u001b[1;32m     62\u001b[0m coalition_estimated_values[\u001b[38;5;28mstr\u001b[39m(coalition)] \u001b[38;5;241m=\u001b[39m impact\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36msend_to_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m data_output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m my_loader:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# 将数据移到指定的设备上（如 CUDA 设备）\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         batch_data \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# 将数据传递给模型进行推理\u001b[39;00m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/hcds_vol/private/luffy/anaconda3/envs/shap_res/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import warnings\n",
    "import itertools\n",
    "# 過濾掉FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "def all_subsets(n_elts):\n",
    "    '''\n",
    "        returns a list of 2^{n_elts} lists\n",
    "        each a different subset of {1, 2,...,n_elts}\n",
    "    '''\n",
    "    res = [np.array(list(itertools.combinations(set(range(n_elts)), i))) for i in range(n_elts)]\n",
    "    res = {i : res[i] for i in range(n_elts)}\n",
    "    res[n_elts] = np.array([i for i in range(n_elts)]).reshape(1,-1)\n",
    "    return [res[i][j] for i in range(1,n_elts+1) for j in range(res[i].shape[0])]\n",
    "para_num = 14\n",
    "AUO_coalitions = [np.array([])] + all_subsets(para_num) \n",
    "coalition_estimated_values = {}\n",
    "instance_idx = 5\n",
    "instance = new_df.iloc[instance_idx]\n",
    "#mean_exp = new_df['Output'].mean()\n",
    "flag=0\n",
    "count_n = 1\n",
    "selected_data = new_df.copy()\n",
    "selected_data = selected_data[(selected_data['PUMP_low']<20000) & \n",
    "                              (selected_data['PUMP_high']>20000) & \n",
    "                              (selected_data['VENT_low']<10000) & \n",
    "                              (selected_data['VENT_high']>10000) &\n",
    "                              (selected_data['NH3_TREAT_-RF_FREQ-max']>13800)&\n",
    "                              (selected_data['NH3_TREAT_-RF_FREQ-mean']<13800)\n",
    "                                                            ]\n",
    "print(len(selected_data))\n",
    "sample_df = selected_data.sample(n=1000,random_state=42)\n",
    "instance.to_csv('instance_3d.csv',index=True)\n",
    "sample_df.to_csv('background_dataset_2.csv',index=True)\n",
    "mean_exp = sample_df['Output'].mean()\n",
    "print(instance['Output'])\n",
    "for coalition in AUO_coalitions:\n",
    "    synth = sample_df.copy()                   #用copy()才不會去更改到原始的dataframe\n",
    "    if len(coalition)!=0:\n",
    "        #print(synth.iloc[:,coalition],instance[coalition])\n",
    "        synth.iloc[:,coalition] = instance.iloc[coalition]\n",
    "        \n",
    "        '''if len(coalition)==3 and flag==0:\n",
    "            print(instance)\n",
    "            print(synth.head(5))\n",
    "            flag=1'''\n",
    "        #if (2 in coalition and 3 not in coalition):\n",
    "        #    PUMP_high = instance.iloc[2]\n",
    "        #    synth = synth[synth.iloc[:,3]<PUMP_high]\n",
    "            #print('good')\n",
    "            \n",
    "    #if count_n==100:\n",
    "        #print('資料集長度:',len(synth))\n",
    "        #print('feature數:',len(coalition))\n",
    "        #print('資料: ',synth)\n",
    "        #print(coalition)\n",
    "        #count_n = 0\n",
    "\n",
    "    count_n += 1\n",
    "    Exp = send_to_model(synth)\n",
    "    impact = Exp - mean_exp\n",
    "\n",
    "    coalition_estimated_values[str(coalition)] = impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUOcube = Hypercube(para_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUOcube.set_vertex_values(coalition_estimated_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instance_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m p_n \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m instance_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_vi_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(instance_folder):\n\u001b[1;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(instance_folder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'instance_idx' is not defined"
     ]
    }
   ],
   "source": [
    "p_n = []\n",
    "instance_folder = f'instance_{instance_idx}_vi_data'\n",
    "if not os.path.exists(instance_folder):\n",
    "    os.makedirs(instance_folder)\n",
    "for i  in range(para_num):\n",
    "    AUOcube.trans_to_matrix(feature_i=i)\n",
    "    res = AUOcube.shapley_residuals_in_matrix()\n",
    "    #print(new_df.columns[i],'residuals_of_feature',i)\n",
    "    save_json(res[0],feature_name=params_list[i],folder_path=instance_folder)\n",
    "    save_res_json(res[1],feature_name=params_list[i],folder_path=instance_folder)\n",
    "    p_n.append(res[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
